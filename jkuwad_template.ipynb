{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZLFJuX3_vID"
   },
   "source": [
    "# Deep Reinforcement Learning — Doom Agent (SS2025)\n",
    "\n",
    "Welcome to the last assignment for the **Deep Reinforcement Learning** course (SS2025). In this notebook, you'll implement and train a reinforcement learning agent to play **Doom**.\n",
    "\n",
    "You will:\n",
    "- Set up a custom VizDoom environment with shaped rewards\n",
    "- Train an agent using an approach of your choice\n",
    "- Track reward components across episodes\n",
    "- Evaluate the best model\n",
    "- Visualize performance with replays and GIFs\n",
    "- Export the trained agent to ONNX to submit to the evaluation server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mi5FBK3oi1lI",
    "outputId": "6a38df99-16cd-4a65-8bb0-ead70efcbd71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'jku.wad'...\n",
      "remote: Enumerating objects: 227, done.\u001b[K\n",
      "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 227 (delta 48), reused 50 (delta 45), pack-reused 169 (from 1)\u001b[K\n",
      "Receiving objects: 100% (227/227), 973.43 KiB | 9.01 MiB/s, done.\n",
      "Resolving deltas: 100% (135/135), done.\n",
      "/Users/lukaskurz/University/deep_reinforcement_learning/assignment4/jku.wad\n"
     ]
    }
   ],
   "source": [
    "# Clone repo\n",
    "!git clone https://$token@github.com/gerkone/jku.wad.git\n",
    "%cd jku.wad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oYTcQU_TiP06",
    "outputId": "9ca3b3f4-26b2-4c80-8aa2-9201e1841cdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: numpy in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: matplotlib in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (3.10.3)\n",
      "Requirement already satisfied: vizdoom in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (1.2.4)\n",
      "Requirement already satisfied: portpicker in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (1.6.0)\n",
      "Requirement already satisfied: gym in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: onnx in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (1.18.0)\n",
      "Requirement already satisfied: filelock in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from vizdoom) (1.1.1)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from vizdoom) (2.6.1)\n",
      "Requirement already satisfied: psutil in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from portpicker) (5.9.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from onnx) (6.31.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from gymnasium>=0.28.0->vizdoom) (0.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lukaskurz/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Install the dependencies\n",
    "!pip install torch numpy matplotlib vizdoom portpicker gym onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "8VR5QUw8ieGh"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Sequence\n",
    "\n",
    "import torch\n",
    "from collections import deque, OrderedDict\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import vizdoom as vzd\n",
    "from vizdoom import ScreenFormat\n",
    "\n",
    "from gym import Env\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "\n",
    "from doom_arena import VizdoomMPEnv\n",
    "from doom_arena.reward import VizDoomReward\n",
    "from doom_arena.render import render_episode\n",
    "from IPython.display import HTML\n",
    "from typing import Dict, Tuple\n",
    "from doom_arena.reward import VizDoomReward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDz9CHjVzP61"
   },
   "source": [
    "## Environment configuration\n",
    "\n",
    "ViZDoom supports multiple visual buffers that can be used as input for training agents. Each buffer provides different information about the game environment, as seen from left to right:\n",
    "\n",
    "\n",
    "Screen\n",
    "- The default first-person RGB view seen by the agent.\n",
    "\n",
    "Labels\n",
    "- A semantic map where each pixel is tagged with an object ID (e.g., enemy, item, wall).\n",
    "\n",
    "Depth\n",
    "- A grayscale map showing the distance from the agent to surfaces in the scene.\n",
    "\n",
    "Automap\n",
    "- A top-down schematic view of the map, useful for global navigation tasks.\n",
    "\n",
    "![buffers gif](https://vizdoom.farama.org/_images/vizdoom-demo.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "nmNDlnmfzP62"
   },
   "outputs": [],
   "source": [
    "USE_GRAYSCALE = False  # ← flip to False for RGB\n",
    "\n",
    "PLAYER_CONFIG = {\n",
    "    # NOTE: \"algo_type\" defaults to POLICY in evaluation script!\n",
    "    \"algo_type\": \"QVALUE\",  # OPTIONAL, change to POLICY if using policy-based (eg PPO)\n",
    "    \"n_stack_frames\": 1,\n",
    "    \"extra_state\": [\"depth\"],\n",
    "    \"hud\": \"none\",\n",
    "    \"crosshair\": True,\n",
    "    \"screen_format\": 8 if USE_GRAYSCALE else 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "A3vdawvQzP62"
   },
   "outputs": [],
   "source": [
    "# TODO: environment training paramters\n",
    "N_STACK_FRAMES = 1\n",
    "NUM_BOTS = 4\n",
    "EPISODE_TIMEOUT = 1000\n",
    "# TODO: model hyperparams\n",
    "GAMMA = 0.99\n",
    "EPISODES = 500\n",
    "BATCH_SIZE = 64\n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "LEARNING_RATE = 3e-4\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 0.999\n",
    "N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLoMT0EMzP63"
   },
   "source": [
    "## Reward function\n",
    "In this task, you will define a reward function to guide the agent's learning. The function is called at every step and receives the current and previous game variables (e.g., number of frags, hits taken, health).\n",
    "\n",
    "Your goal is to combine these into a meaningful reward, encouraging desirable behavior, such as:\n",
    "\n",
    "- Rewarding frags (enemy kills)\n",
    "\n",
    "- Rewarding accuracy (hitting enemies)\n",
    "\n",
    "- Penalizing damage taken\n",
    "\n",
    "- (Optional) Encouraging survival, ammo efficiency, etc.\n",
    "\n",
    "You can return multiple reward components, which are summed during training. Consider the class below as a great starting point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Gq1xlneLzP63"
   },
   "outputs": [],
   "source": [
    "class YourReward(VizDoomReward):\n",
    "    def __init__(self, num_players: int):\n",
    "        super().__init__(num_players)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        vizdoom_reward: float,\n",
    "        game_var: Dict[str, float],\n",
    "        game_var_old: Dict[str, float],\n",
    "        player_id: int,\n",
    "    ) -> Tuple[float, float, float, float]:  # Added one more component\n",
    "        \"\"\"\n",
    "        Enhanced reward function:\n",
    "        * +100  for every new frag\n",
    "        * +2    for every hit landed\n",
    "        * -0.1  for every hit taken\n",
    "        * +0.1  for survival (small positive reward per step)\n",
    "        \"\"\"\n",
    "        self._step += 1\n",
    "        _ = vizdoom_reward, player_id  # unused\n",
    "\n",
    "        rwd_hit = 2.0 * (game_var[\"HITCOUNT\"] - game_var_old[\"HITCOUNT\"])\n",
    "        rwd_hit_taken = -0.1 * (game_var[\"HITS_TAKEN\"] - game_var_old[\"HITS_TAKEN\"])\n",
    "        rwd_frag = 100.0 * (game_var[\"FRAGCOUNT\"] - game_var_old[\"FRAGCOUNT\"])\n",
    "        rwd_survival = 0.1  # Small positive reward for staying alive\n",
    "\n",
    "        return rwd_hit, rwd_hit_taken, rwd_frag, rwd_survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzTS4m2VzP63",
    "outputId": "369317da-3265-4f5c-b2f1-7990ca7d1f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host 51555\n",
      "Player 51555\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "DTYPE = torch.float32\n",
    "\n",
    "reward_fn = YourReward(num_players=1)\n",
    "\n",
    "env = VizdoomMPEnv(\n",
    "    num_players=1,\n",
    "    num_bots=NUM_BOTS,\n",
    "    bot_skill=0,\n",
    "    doom_map=\"ROOM\",  # NOTE simple, small map; other options: TRNM, TRNMBIG\n",
    "    extra_state=PLAYER_CONFIG[\n",
    "        \"extra_state\"\n",
    "    ],  # see info about states at the beginning of 'Environment configuration' above\n",
    "    episode_timeout=EPISODE_TIMEOUT,\n",
    "    n_stack_frames=PLAYER_CONFIG[\"n_stack_frames\"],\n",
    "    crosshair=PLAYER_CONFIG[\"crosshair\"],\n",
    "    hud=PLAYER_CONFIG[\"hud\"],\n",
    "    screen_format=PLAYER_CONFIG[\"screen_format\"],\n",
    "    reward_fn=reward_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkuIefaSzP64"
   },
   "source": [
    "## Agent\n",
    "\n",
    "Implement **your own agent** in the code cell that follows.\n",
    "\n",
    "* In `agents/dqn.py` and `agents/ppo.py` you’ll find very small **skeletons**—they compile but are meant only as reference or quick tests.  \n",
    "  Feel free to open them, borrow ideas, extend them, or ignore them entirely.\n",
    "* The notebook does **not** import those files automatically; whatever class you define in the next cell is the one that will be trained.\n",
    "* You may keep the DQN interface, switch to PPO, or try something else.\n",
    "* Tweak any hyper-parameters (`PLAYER_CONFIG`, ε-schedule, optimiser, etc.) and document what you tried.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "pP6UxV5NzP64"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# DQN — design your network here\n",
    "# ================================================================\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim: int, action_space: int, hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, 32, 8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),       nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1),       nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 12 * 12, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, action_space),\n",
    "        )\n",
    "\n",
    "    def forward(self, frame: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(frame)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "n7dWjwTdzP65"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Initialise your networks and training utilities\n",
    "# ================================================================\n",
    "\n",
    "# main Q-network\n",
    "in_channels = env.observation_space.shape[0]  # 1 if grayscale, else 3/4\n",
    "model = DQN(\n",
    "    input_dim=in_channels,\n",
    "    action_space=env.action_space.n,\n",
    "    hidden=64,  # change or ignore\n",
    ").to(device, dtype=DTYPE)\n",
    "\n",
    "# Target network (for stable Q-learning)\n",
    "model_tgt = deepcopy(model).to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "# Replay buffer\n",
    "replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)\n",
    "\n",
    "# Initial epsilon for epsilon-greedy exploration\n",
    "epsilon = EPSILON_START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(env, model, obs, epsilon, device, dtype):\n",
    "    \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = obs.unsqueeze(0).to(device, dtype=dtype)\n",
    "            q_values = model(obs_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "def update_ema(target_model, online_model, tau=0.005):\n",
    "    \"\"\"Exponential moving average update for target network\"\"\"\n",
    "    for target_param, online_param in zip(target_model.parameters(), online_model.parameters()):\n",
    "        target_param.data.copy_(tau * online_param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mihj8Q5xzP65"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "W5V4XMeIk5Uv"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (64x9216 and 13056x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m r = torch.tensor(r, device=device, dtype=torch.float32)\n\u001b[32m     44\u001b[39m d = torch.tensor(d, device=device, dtype=torch.float32)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m q = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m.gather(\u001b[32m1\u001b[39m, a.unsqueeze(\u001b[32m1\u001b[39m)).squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     48\u001b[39m     q2 = model_tgt(s2).max(\u001b[32m1\u001b[39m).values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mDQN.forward\u001b[39m\u001b[34m(self, frame)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame: torch.Tensor) -> torch.Tensor:\n\u001b[32m     24\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.encoder(frame)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/deep_reinforcement_learning/assignment4/.conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: linear(): input and weight.T shapes cannot be multiplied (64x9216 and 13056x64)"
     ]
    }
   ],
   "source": [
    "# ---------------------  TRAINING LOOP  ----------------------\n",
    "# Feel free to change EVERYTHING below:\n",
    "#   • choose your own reward function\n",
    "#   • track different episode statistics in `ep_metrics`\n",
    "#   • switch optimiser, scheduler, update rules, etc.\n",
    "\n",
    "reward_list, q_loss_list = [], []\n",
    "best_eval_return, best_model = float(\"-inf\"), None\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    ep_metrics = {\"custom_reward\": 0.0}  # ← add or replace keys as you like\n",
    "    obs = env.reset()[0]\n",
    "    done, ep_return = False, 0.0\n",
    "    model.eval()\n",
    "\n",
    "    # ───────── rollout ─────────────────────────────────────────────\n",
    "    while not done:\n",
    "        act = epsilon_greedy(env, model, obs, epsilon, device, DTYPE)\n",
    "        next_obs, rwd_raw, done, _ = env.step(act)\n",
    "\n",
    "        # ----- reward definition (EDIT here) ----------------\n",
    "        custom_rwd = float(rwd_raw[0])  # default: raw env reward\n",
    "        # Example: access game variables for more detailed reward engineering\n",
    "        # gv, gv_pre = env.envs[0].unwrapped._game_vars, env.envs[0].unwrapped._game_vars_pre\n",
    "        # custom_rwd = your_function(gv, gv_pre)\n",
    "\n",
    "        ep_metrics[\"custom_reward\"] += custom_rwd\n",
    "\n",
    "        replay_buffer.append((obs, act, custom_rwd, next_obs[0], done))\n",
    "        obs, ep_return = next_obs[0], ep_return + custom_rwd\n",
    "    reward_list.append(ep_return)\n",
    "\n",
    "    # ───────── learning step (experience replay) ──────────────────\n",
    "    if len(replay_buffer) >= BATCH_SIZE:\n",
    "        model.train()\n",
    "        for _ in range(N_EPOCHS):\n",
    "            batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            s, a, r, s2, d = zip(*batch)\n",
    "\n",
    "            s = torch.stack(s).to(device, dtype=DTYPE)\n",
    "            s2 = torch.stack(s2).to(device, dtype=DTYPE)\n",
    "            a = torch.tensor(a, device=device)\n",
    "            r = torch.tensor(r, device=device, dtype=torch.float32)\n",
    "            d = torch.tensor(d, device=device, dtype=torch.float32)\n",
    "\n",
    "            q = model(s).gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "            with torch.no_grad():\n",
    "                q2 = model_tgt(s2).max(1).values\n",
    "                tgt = r + GAMMA * q2 * (1 - d)\n",
    "            loss = F.mse_loss(q, tgt)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            q_loss_list.append(loss.item())\n",
    "        update_ema(model_tgt, model)\n",
    "\n",
    "    scheduler.step()\n",
    "    epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
    "    print(f\"Ep {episode+1:03}: return {ep_return:6.1f}  |  ε {epsilon:.3f}\")\n",
    "\n",
    "    # ───────── quick evaluation for best-model tracking ───────────\n",
    "    eval_obs, done, eval_return = env.reset()[0], False, 0.0\n",
    "    model.eval()\n",
    "    while not done:\n",
    "        act = epsilon_greedy(env, model, eval_obs, 0.05, device, DTYPE)\n",
    "        eval_obs_n, r, done, _ = env.step(act)\n",
    "        eval_obs = eval_obs_n[0]\n",
    "        eval_return += r[0]\n",
    "    if eval_return > best_eval_return:\n",
    "        best_eval_return, best_model = eval_return, deepcopy(model)\n",
    "\n",
    "# ---------------------  SAVE / EXPORT ---------------------------------------\n",
    "final_model = best_model if best_model is not None else model  # choose best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpBx9O9yzP66"
   },
   "source": [
    "## Dump to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNkgBbUSzP66"
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "import json\n",
    "\n",
    "\n",
    "def onnx_dump(env, model, config, filename: str):\n",
    "    # dummy state\n",
    "    init_state = env.reset()[0].unsqueeze(0)\n",
    "\n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        model.cpu(),\n",
    "        args=init_state,\n",
    "        f=filename,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    )\n",
    "    onnx_model = onnx.load(filename)\n",
    "\n",
    "    meta = onnx_model.metadata_props.add()\n",
    "    meta.key = \"config\"\n",
    "    meta.value = json.dumps(config)\n",
    "\n",
    "    onnx.save(onnx_model, filename)\n",
    "\n",
    "\n",
    "onnx_dump(env, final_model, PLAYER_CONFIG, filename=\"model.onnx\")\n",
    "print(\"Best network exported to model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rZCsfQHFMu-"
   },
   "source": [
    "### Evaluation and Visualization\n",
    "\n",
    "In this final section, you can evaluate your trained agent, inspect its performance visually, and analyze reward components over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARC2K2k686nu"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "#  Reward-plot helper  (feel free to edit / extend)\n",
    "# ---------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_reward_components(reward_log, smooth_window: int = 5):\n",
    "    \"\"\"\n",
    "    Plot raw and smoothed episode-level reward components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reward_log : list[dict]\n",
    "        Append a dict for each episode, e.g. {\"frag\": …, \"hit\": …, \"hittaken\": …}\n",
    "    smooth_window : int\n",
    "        Rolling-mean window size for the smoothed curve.\n",
    "    \"\"\"\n",
    "    if not reward_log:\n",
    "        print(\"reward_log is empty – nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(reward_log)\n",
    "    df_smooth = df.rolling(window=smooth_window, min_periods=1).mean()\n",
    "\n",
    "    # raw\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for col in df.columns:\n",
    "        plt.plot(df.index, df[col], label=col)\n",
    "    plt.title(\"Raw episode reward components\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # smoothed\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for col in df.columns:\n",
    "        plt.plot(df.index, df_smooth[col], label=f\"{col} (avg)\")\n",
    "    plt.title(f\"Smoothed (window={smooth_window})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# Hint for replay visualisation:\n",
    "# ----------------------------------------------------------------\n",
    "# env.enable_replay()\n",
    "# ... run an evaluation episode ...\n",
    "# env.disable_replay()\n",
    "# replays = env.get_player_replays()\n",
    "#\n",
    "# from doom_arena.render import render_episode\n",
    "# from IPython.display import HTML\n",
    "# HTML(render_episode(replays, subsample=5).to_html5_video())\n",
    "#\n",
    "# Feel free to adapt or write your own GIF/MP4 export.\n",
    "\n",
    "# load onnx model\n",
    "\n",
    "from server_eval_doom import ConvertModel\n",
    "from agents.dqn import epsilon_greedy\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_file = \"model.onnx\"\n",
    "model = onnx.load(model_file)\n",
    "config = next(\n",
    "    (json.loads(p.value) for p in model.metadata_props if p.key == \"config\"), {}\n",
    ")\n",
    "model = ConvertModel(model)\n",
    "model.eval()\n",
    "model = model.to(DEVICE, dtype=DTYPE)\n",
    "\n",
    "env.enable_replay()\n",
    "obs_v, done_v = env.reset()[0], False\n",
    "while not done_v:\n",
    "    a_v = epsilon_greedy(env, model, obs_v, 0.0, device, DTYPE)\n",
    "    obs_n_v, _, done_v, _ = env.step(a_v)\n",
    "    obs_v = obs_n_v[0]\n",
    "env.disable_replay()\n",
    "\n",
    "replays = env.get_player_replays()\n",
    "anim = render_episode(replays, subsample=5)  # FuncAnimation\n",
    "# save as MP4 via ffmpeg writer\n",
    "filename = f\"replay.mp4\"\n",
    "anim.save(filename, writer='ffmpeg', fps=27)\n",
    "print(f\"Saved replay to {filename}\")\n",
    "# display inline\n",
    "display(HTML(anim.to_html5_video()))\n",
    "print()  # spacer\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mhd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
